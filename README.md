# Fine-tuning **Gemma-3 Instruct (1 B)** on *MTS-Dialog* with LoRA

An end-to-end, reproducible workflow that trains a 1 B-parameter Gemma model to
turn raw doctor-patient conversations into concise clinical notes.  
Everything happens in a single notebook / script:

* **LoRA rank 16**, **3 epochs** ‚Äì fits on a single ‚âà16 GB T4.
* **Mixed-precision (`bfloat16`)** for extra memory headroom. 
* **EarlyStopping on `val_loss`** to avoid over-training.  
* Post-training **ROUGE evaluation** and a small **Playground** for manual testing.

---

## üèÅ Quick start (Colab)
	1.	Open the notebook in Colab (File ‚ñ∏ Open Notebook ‚ñ∏ ‚ÄúUpload‚Äù and choose gemma_medical_finetuning.ipynb).
	2.	Set the runtime to GPU (Runtime ‚ñ∏ Change runtime type ‚ñ∏ GPU). A T4 or better (‚âà 16 GB VRAM) is enough.
	3.	Install dependencies ‚Äì simply run the first code cell:

!pip install -q -U tensorflow keras keras-nlp keras-hub rouge_score scipy tqdm ipywidgets


	4.	Load utils.py into Colab (needed by the notebook):

### ‚ñ∂Ô∏é Option A ‚Äî clone the repo (preferred, keeps everything in sync)
!git clone https://github.com/your-handle/gemma-mts-dialog.git
%cd gemma-mts-dialog

### ‚ñ∂Ô∏é Option B ‚Äî manual upload (if you don‚Äôt want to clone)
from google.colab import files
files.upload()   # then pick utils.py from your computer

The notebook expects utils.py to be in the current working directory; either option is fine.

	5.	Run all cells. The notebook pulls the MTS-Dialog CSVs automatically and starts training.
After ‚âà 45 min a file named lora_rank_16.weights.lora.h5 will appear ‚Äî that‚Äôs your LoRA adapter, ready for inference.

---

## ‚ö†Ô∏è Local execution

This tutorial is written and tested for Google Colab.
Running it locally is untested and may require manual tweaks (CUDA setup, memory tweaks, custom dataset paths). If you do attempt it, please treat the process as experimental.

---

## Files

| File                                | Purpose                                                |
| ----------------------------------- | ------------------------------------------------------ |
| `gemma_medical_finetuning.py`       | Full training / evaluation / demo pipeline             |
| `utils.py`                          | Main functions used in the notebook                    |
| `lora_rank16.weights.lora.h5`       | LoRA adapter weights (created after training)          |
| `requirements.txt`                  | Exact Python packages & versions (if running locally)  |

---

## üìú Dataset licence

*MTS-Dialog* ¬© 2023 ‚Äì released under **CC BY 4.0** (see `LICENSE-DATA`)
Generated clinical notes are purely **research output**; they are **not** meant
for real-world medical use without additional safety checks.

---

## üôè Credits

* Google AI for releasing **Gemma-3** and the official LoRA tutorial.
* Original MTS-Dialog creators for the dataset.
* This repo authored & cleaned-up by **@BryenInsights** (June 2025).
