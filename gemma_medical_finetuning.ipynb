{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "aac98f21",
      "metadata": {
        "id": "aac98f21"
      },
      "source": [
        "# Fineâ€‘tuning **Gemmaâ€‘3 Instruct (1â€¯B)** on the *MTSâ€‘Dialog* medicalâ€‘conversation dataset  \n",
        "\n",
        "*An endâ€‘toâ€‘end, LoRAâ€‘based workflow with evaluation & demo*  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "68e16b78",
      "metadata": {
        "id": "68e16b78"
      },
      "source": [
        "**Provenance / Credits**\n",
        "\n",
        "* Originally authored in GoogleÂ Colab.  \n",
        "* Refactored and cleaned up for GitHub readability â€“ MayÂ 2025.  \n",
        "* Dataset: **MTSâ€‘Dialog** (Â©Â 2024, MITâ€‘licensed)  \n",
        "* Model: **Gemmaâ€‘3 InstructÂ 1â€¯B** via `kerasâ€‘hub`.  \n",
        "\n",
        "> The notebook assumes GoogleÂ Colab **or** a local machine with a modern GPU and official Gemma access.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "f94a756a",
      "metadata": {
        "id": "f94a756a"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# â”€â”€â”€ Environment setup â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# Feel free to skip if these libraries are already installed.\n",
        "!pip install -q -U keras-hub keras keras-nlp rouge_score scipy tqdm ipywidgets\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d8b1dd2",
      "metadata": {
        "id": "6d8b1dd2"
      },
      "source": [
        "## ğŸ”‘ Reproducibility â€“ set a single global RNG seed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "44a8ef49",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "44a8ef49",
        "outputId": "8adee39c-63a1-42c6-ff93-4d51a146fa9c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Seed set to 42\n"
          ]
        }
      ],
      "source": [
        "import os, random, gc, json, textwrap\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "def set_global_seed(seed: int = 42):\n",
        "    \"\"\"Seed Python, NumPy, TF & Keras RNGs (see TFÂ docs for details).\"\"\"\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    tf.random.set_seed(seed)\n",
        "    tf.keras.utils.set_random_seed(seed)\n",
        "    print(f\"âœ… Seed set to {seed}\")\n",
        "\n",
        "SEED = 42\n",
        "set_global_seed(SEED)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "b127f1b4",
      "metadata": {
        "id": "b127f1b4"
      },
      "outputs": [],
      "source": [
        "# â”€â”€â”€ Core libraries â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "import os, random, gc, pprint, itertools, math, time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from scipy import stats\n",
        "from rouge_score import rouge_scorer\n",
        "import torch\n",
        "\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "import keras_hub\n",
        "import keras_nlp\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "# â¬‡ï¸  Optional: Colabâ€‘only helper to pull your Kaggle creds from the browser\n",
        "try:\n",
        "    from google.colab import userdata  # type: ignore\n",
        "    os.environ[\"KAGGLE_USERNAME\"] = userdata.get(\"KAGGLE_USERNAME\")\n",
        "    os.environ[\"KAGGLE_KEY\"] = userdata.get(\"KAGGLE_KEY\")\n",
        "except (ImportError, ModuleNotFoundError):\n",
        "    print(\"Not on Colab â€“ set $KAGGLE_USERNAME and $KAGGLE_KEY manually if needed.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b339150a",
      "metadata": {
        "id": "b339150a"
      },
      "source": [
        "## ğŸ“ Prompt template (single definition â€“ DRY)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "d60392af",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d60392af",
        "outputId": "5dfa2e9f-ad60-44f3-b64d-38a49e1a328c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<start_of_turn>user\n",
            "\n",
            "Instruction:\n",
            "Summarize the following doctorâ€“patient conversation into a concise past-tense clinical note beginning with â€˜The patientâ€¦â€™. Include all medically relevant facts, especially symptoms, diagnoses, medications (with dosages), and procedures **if mentioned**. Do not mention categories that are not discussed. Omit small talk or irrelevant details. If there is nothing clinically relevant, respond with â€˜None.â€™\n",
            "\n",
            "Doctor-Patient Dialogue:\n",
            "{dialogue}\n",
            "<end_of_turn>\n",
            "\n",
            "<start_of_turn>model\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Using Gemma's native chat format for best performance\n",
        "TEMPLATE = \"\"\"<start_of_turn>user\n",
        "\n",
        "Instruction:\n",
        "{instruction}\n",
        "\n",
        "Doctor-Patient Dialogue:\n",
        "{dialogue}\n",
        "<end_of_turn>\n",
        "\n",
        "<start_of_turn>model\n",
        "\"\"\"\n",
        "\n",
        "instruction = (\n",
        "    \"Summarize the following doctorâ€“patient conversation into a concise past-tense clinical note beginning with â€˜The patientâ€¦â€™. \"\n",
        "    \"Include all medically relevant facts, especially symptoms, diagnoses, medications (with dosages), and procedures **if mentioned**. \"\n",
        "    \"Do not mention categories that are not discussed. Omit small talk or irrelevant details. \"\n",
        "    \"If there is nothing clinically relevant, respond with â€˜None.â€™\"\n",
        ")\n",
        "\n",
        "formatted_template = TEMPLATE.format(instruction=instruction, dialogue=\"{dialogue}\")\n",
        "print(formatted_template)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "I_uzV10uYUNJ",
      "metadata": {
        "id": "I_uzV10uYUNJ"
      },
      "source": [
        "## ğŸ”¢ Token length utilities for data filtering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "SYE4ElOqYR4K",
      "metadata": {
        "id": "SYE4ElOqYR4K"
      },
      "outputs": [],
      "source": [
        "def estimate_tokens(text: str) -> int:\n",
        "    \"\"\"Rough token estimate (1 token â‰ˆ 4 characters for most models).\"\"\"\n",
        "    return len(text) // 4\n",
        "\n",
        "def filter_data_by_length(\n",
        "    prompts,\n",
        "    responses,\n",
        "    max_input_tokens: int = 1500,\n",
        "    min_prompt_len: int = 10,\n",
        "    min_response_len: int = 10,\n",
        "    end_token: str = \"<end_of_turn>\",\n",
        "):\n",
        "    \"\"\"\n",
        "    Clean + length-filter prompt/response pairs.\n",
        "    \"\"\"\n",
        "    assert len(prompts) == len(responses), \"Prompts and responses must align\"\n",
        "\n",
        "    filtered_prompts, filtered_responses = [], []\n",
        "\n",
        "    # Drop pairs with None or empty strings.\n",
        "    for prompt, response in zip(prompts, responses):\n",
        "        if not prompt or not response:              # None or empty string\n",
        "            continue\n",
        "\n",
        "        prompt, response = prompt.strip(), response.strip()\n",
        "        # Keep samples whose character lengths meet `min_*_len` and `max_input_tokens`\n",
        "        if (\n",
        "            len(prompt) < min_prompt_len\n",
        "            or len(response) < min_response_len\n",
        "            or estimate_tokens(prompt) > max_input_tokens\n",
        "        ):\n",
        "            continue\n",
        "        # Append `end_token`\n",
        "        if not response.endswith(end_token):\n",
        "            response += f\"\\n{end_token}\"\n",
        "\n",
        "        filtered_prompts.append(prompt)\n",
        "        filtered_responses.append(response)\n",
        "\n",
        "    kept = len(filtered_prompts)\n",
        "    total = len(prompts)\n",
        "    print(f\"ğŸ“Š Kept {kept:,} / {total:,} pairs ({kept/total*100:.1f}%)\")\n",
        "\n",
        "    return filtered_prompts, filtered_responses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "8e0aead4",
      "metadata": {
        "id": "8e0aead4"
      },
      "outputs": [],
      "source": [
        "def compile_with_sampler(model, k: int = 5, seed: int = 42):\n",
        "    \"\"\"Attach a deterministic Topâ€‘K sampler & compile the model.\"\"\"\n",
        "    sampler = keras_nlp.samplers.TopKSampler(k=k, seed=seed)\n",
        "    model.compile(sampler=sampler)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "46bb9dbc",
      "metadata": {
        "id": "46bb9dbc"
      },
      "source": [
        "### ğŸ” Quick sanityâ€‘check inference with the *base* Gemma model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "99fb90c6",
      "metadata": {
        "id": "99fb90c6"
      },
      "outputs": [],
      "source": [
        "# Free GPU memory before loading base model (prevents OOM errors)\n",
        "\n",
        "if 'base_model' in globals():\n",
        "    del base_model\n",
        "    gc.collect()\n",
        "    tf.keras.backend.clear_session()\n",
        "\n",
        "base_model = keras_hub.models.Gemma3CausalLM.from_preset(\"gemma3_instruct_1b\")\n",
        "compile_with_sampler(base_model)\n",
        "\n",
        "sample_dialogue = \"\"\"Doctor: How are you feeling today?\n",
        "Patient: I'm still having chest pains.\n",
        "Doctor: Are they sharp or dull?\n",
        "Patient: More of a crushing pressure right here.\"\"\"\n",
        "\n",
        "prompt = TEMPLATE.format(\n",
        "    instruction=instruction,\n",
        "    dialogue='\"\"\"' + sample_dialogue + '\"\"\"'\n",
        ")\n",
        "\n",
        "print(base_model.generate(prompt, max_length=512))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2458620d",
      "metadata": {
        "id": "2458620d"
      },
      "source": [
        "## ğŸ“š Load & prepare the *MTSâ€‘Dialog* training set\n",
        "\n",
        "*MTSâ€‘Dialog* (`1.7â€¯k` doctorâ€“patient conversations with expert summaries).  \n",
        "Below we:  \n",
        "1. Fetch or load the CSV  \n",
        "2. (Optionally) **sample 500 rows** for a quick LoRA demo  \n",
        "3. Wrap dialogue text in triple quotes to preserve line breaks  \n",
        "4. Build two parallel lists `prompts` and `responses` expected by `kerasâ€‘hub`  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "f98abd29",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f98abd29",
        "outputId": "e74fecd7-cc47-4a97-de92-cf44dc2d84db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ’¾ Loaded dataset with 1,201 rows\n",
            "ğŸ“Š Kept 1,115 / 1,121 pairs (99.5%)\n",
            "âœ… Prepared 1,115 filtered prompt/response pairs\n"
          ]
        }
      ],
      "source": [
        "DATA_PATH = \"/content/MTS-Dialog-TrainingSet.csv\"  # adjust if running locally\n",
        "\n",
        "df = pd.read_csv(DATA_PATH)\n",
        "print(f\"ğŸ’¾ Loaded dataset with {len(df):,} rows\")\n",
        "\n",
        "# --- speedâ€‘up during experimentation ---\n",
        "# df = df.sample(n=500, random_state=SEED)  # commentâ€‘out for full training\n",
        "\n",
        "# --- minimal preprocessing ---\n",
        "df = df.dropna(subset=[\"dialogue\", \"section_text\"])\n",
        "\n",
        "# Remove rows where 'section_text' is very short\n",
        "df = df.query(\"section_text.str.len() > 10\", engine=\"python\")\n",
        "\n",
        "# Trim whitespace and add end token to 'section_text'\n",
        "df[\"section_text\"] = df[\"section_text\"].str.strip() + \"\\n<end_of_turn>\"\n",
        "\n",
        "df['dialogue'] = '\"\"\"' + df['dialogue'] + '\"\"\"'\n",
        "\n",
        "prompts, responses = [], []\n",
        "for row in df.itertuples(index=False):\n",
        "    prompts.append(\n",
        "        TEMPLATE.format(instruction=instruction, dialogue=row.dialogue)\n",
        "    )\n",
        "    responses.append(row.section_text)\n",
        "\n",
        "# Stage 2: Token-length filtering (after prompt formatting)\n",
        "\n",
        "prompts, responses = filter_data_by_length(prompts, responses, max_input_tokens=1024)\n",
        "\n",
        "data = {'prompts': prompts, 'responses': responses}\n",
        "print(f\"âœ… Prepared {len(prompts):,} filtered prompt/response pairs\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11ac89f7",
      "metadata": {
        "id": "11ac89f7"
      },
      "source": [
        "## ğŸ”§ LoRA fineâ€‘tuning\n",
        "\n",
        "> **Tip:** a single T4Â GPU needs ~8â€¯GB for rankâ€¯8.  \n",
        "> Mixedâ€‘precision (`keras.mixed_precision.set_global_policy('mixed_bfloat16')`)  \n",
        "> can further reduce memory but is optional.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "7abd9bc8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7abd9bc8",
        "outputId": "8f2d07b4-cc30-4bee-be6c-d97acd533a65"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ”¹ Train samples: 1004\n",
            "ğŸ”¹ Val   samples: 111\n",
            "Epoch 1/5\n",
            "\u001b[1m502/502\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1487s\u001b[0m 3s/step - loss: 0.2083 - sparse_categorical_accuracy: 0.5058 - val_loss: 0.1358 - val_sparse_categorical_accuracy: 0.5562\n",
            "Epoch 2/5\n",
            "\u001b[1m502/502\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1325s\u001b[0m 3s/step - loss: 0.1319 - sparse_categorical_accuracy: 0.5541 - val_loss: 0.1266 - val_sparse_categorical_accuracy: 0.5776\n",
            "Epoch 3/5\n",
            "\u001b[1m502/502\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1273s\u001b[0m 3s/step - loss: 0.1245 - sparse_categorical_accuracy: 0.5716 - val_loss: 0.1229 - val_sparse_categorical_accuracy: 0.5892\n",
            "Epoch 4/5\n",
            "\u001b[1m502/502\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1273s\u001b[0m 3s/step - loss: 0.1208 - sparse_categorical_accuracy: 0.5800 - val_loss: 0.1208 - val_sparse_categorical_accuracy: 0.5918\n",
            "Epoch 5/5\n",
            "\u001b[1m502/502\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1296s\u001b[0m 3s/step - loss: 0.1184 - sparse_categorical_accuracy: 0.5852 - val_loss: 0.1192 - val_sparse_categorical_accuracy: 0.5959\n"
          ]
        }
      ],
      "source": [
        "# Clean start â€“ free GPU RAM if notebook was reâ€‘run\n",
        "if 'gemma_lm' in globals():\n",
        "    del gemma_lm\n",
        "    gc.collect()\n",
        "    tf.keras.backend.clear_session()\n",
        "\n",
        "# Mixed precision\n",
        "keras.mixed_precision.set_global_policy('mixed_bfloat16')\n",
        "\n",
        "# Load model WITH pre-trained weights, then enable LoRA\n",
        "gemma_lm = keras_hub.models.Gemma3CausalLM.from_preset(\"gemma3_instruct_1b\")  # load_weights=True by default\n",
        "gemma_lm.backbone.enable_lora(rank=8)  # rank=8: balance between adaptation capacity and efficiency\n",
        "\n",
        "compile_with_sampler(gemma_lm, k=5, seed=SEED)\n",
        "\n",
        "gemma_lm.summary\n",
        "\n",
        "early_stop = EarlyStopping(\n",
        "    monitor=\"val_loss\",\n",
        "    patience=2,\n",
        "    restore_best_weights=False,\n",
        ")\n",
        "\n",
        "VAL_FRACTION = 0.10\n",
        "rng = np.random.default_rng(SEED)\n",
        "\n",
        "prompts_arr   = np.array(data[\"prompts\"],    dtype=object)\n",
        "responses_arr = np.array(data[\"responses\"],  dtype=object)\n",
        "\n",
        "idx           = rng.permutation(len(prompts_arr))\n",
        "val_size      = int(len(idx) * VAL_FRACTION)\n",
        "\n",
        "val_idx, train_idx = idx[:val_size], idx[val_size:]\n",
        "\n",
        "train_data = {\n",
        "    \"prompts\":   prompts_arr[train_idx],\n",
        "    \"responses\": responses_arr[train_idx],\n",
        "}\n",
        "val_data = {\n",
        "    \"prompts\":   prompts_arr[val_idx],\n",
        "    \"responses\": responses_arr[val_idx],\n",
        "}\n",
        "\n",
        "print(f\"ğŸ”¹ Train samples: {len(train_idx)}\")\n",
        "print(f\"ğŸ”¹ Val   samples: {len(val_idx)}\")\n",
        "\n",
        "history = gemma_lm.fit(\n",
        "    train_data,\n",
        "    validation_data=val_data,   # 10 % of the batches become validation data\n",
        "    epochs=2,\n",
        "    batch_size=1,\n",
        "    callbacks=[early_stop],\n",
        "    shuffle=True,            # keeps training / validation split random each run\n",
        ")\n",
        "\n",
        "# Save the adapters so the model can be re-used elsewhere\n",
        "gemma_lm.backbone.save_lora_weights(\"lora_rank8.weights.lora.h5\")\n",
        "\n",
        "# Note : We will reuse the adapter later in the Playground section"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e8160015",
      "metadata": {
        "id": "e8160015"
      },
      "source": [
        "### ğŸ“ˆ Training curves"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "290dc41b",
      "metadata": {
        "id": "290dc41b"
      },
      "outputs": [],
      "source": [
        "loss = history.history.get('loss', [])\n",
        "acc  = history.history.get('accuracy', [])\n",
        "\n",
        "epochs = range(1, len(loss)+1)\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.plot(epochs, loss, marker='o', label='Loss')\n",
        "if acc:\n",
        "    plt.plot(epochs, acc, marker='o', label='Accuracy')\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.xticks(epochs)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.title(\"Gemma LoRA fineâ€‘tuning progress\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f93302e1",
      "metadata": {
        "id": "f93302e1"
      },
      "source": [
        "## ğŸ› ï¸ Utility functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7da93c1a",
      "metadata": {
        "id": "7da93c1a"
      },
      "outputs": [],
      "source": [
        "def safe_generate(model, prompt, max_new_tokens: int = 4_000, strip_prompt=True, **kw):\n",
        "    \"\"\"\n",
        "    Length-safe wrapper around Gemma3CausalLM.generate().\n",
        "    Keeps the public API identical to Hugging-Face helpers\n",
        "    (which accept `max_new_tokens`) while calling Keras-Hub\n",
        "    with the correct arguments.\n",
        "    \"\"\"\n",
        "    # Tokenised prompt length\n",
        "    prompt_len = len(\n",
        "        model.preprocessor.generate_preprocess([prompt])[\"token_ids\"][0]\n",
        "    )\n",
        "\n",
        "    # Modelâ€™s absolute limit\n",
        "    ceiling = getattr(\n",
        "        getattr(model, \"config\", None), \"max_position_embeddings\", 8_192\n",
        "    )\n",
        "\n",
        "    if prompt_len >= ceiling:\n",
        "        raise ValueError(\n",
        "            f\"Prompt is {prompt_len} tokens but Gemma can accept at most {ceiling}.\"\n",
        "        )\n",
        "\n",
        "    # Clip the *new* tokens so total â‰¤ ceiling\n",
        "    allowed_new  = min(max_new_tokens, ceiling - prompt_len)\n",
        "    total_length = prompt_len + allowed_new\n",
        "\n",
        "    # Make sure we DONâ€™T pass the unsupported kwarg downstream\n",
        "    kw.pop(\"max_new_tokens\", None)\n",
        "    kw[\"max_length\"] = total_length\n",
        "\n",
        "    # Optional: forward strip_prompt if the caller set it\n",
        "    return model.generate(prompt, strip_prompt=strip_prompt, **kw)\n",
        "\n",
        "def evaluate_model(model, prompts, references, model_name: str = \"model\"):\n",
        "    # Evaluate using ROUGE metrics (standard for summarization tasks)\n",
        "    # ROUGE-1/2: n-gram overlap, ROUGE-L: longest common subsequence\n",
        "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "    scores = {'rouge1': [], 'rouge2': [], 'rougeL': []}\n",
        "    outputs = []\n",
        "\n",
        "    for prompt in tqdm(prompts, desc=f\"Generating with {model_name}\"):\n",
        "        outputs.append(safe_generate(model, prompt, max_new_tokens=3000))\n",
        "\n",
        "    for hyp, ref in zip(outputs, references):\n",
        "        rouge = scorer.score(ref, hyp)\n",
        "        for k in scores:\n",
        "            scores[k].append(rouge[k].fmeasure)\n",
        "\n",
        "    metrics = {k: np.mean(v) for k, v in scores.items()}\n",
        "    return outputs, scores, metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d5ebd708",
      "metadata": {
        "id": "d5ebd708"
      },
      "source": [
        "## ğŸ”¬ Validation set evaluation\n",
        "\n",
        "We compare the *base* and *fineâ€‘tuned* checkpoints on the MTSâ€‘Dialog validation split.  \n",
        "*(~25â€¯min on a T4, ~8â€¯min on an A100 â€“ feel free to skip in a hurry.)*  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "12c2712e",
      "metadata": {
        "id": "12c2712e"
      },
      "outputs": [],
      "source": [
        "VAL_PATH = \"/content/MTS-Dialog-ValidationSet.csv\"\n",
        "val_df = pd.read_csv(VAL_PATH)\n",
        "\n",
        "# Optional speed-up during experimentation\n",
        "val_df = val_df.sample(n=50, random_state=SEED)\n",
        "\n",
        "# Match the training-time preprocessing\n",
        "val_df[\"dialogue\"] = '\"\"\"' + val_df[\"dialogue\"] + '\"\"\"'\n",
        "\n",
        "prompts_val, references_val = [], []\n",
        "for row in val_df.itertuples(index=False):\n",
        "    prompts_val.append(TEMPLATE.format(instruction=instruction, dialogue=row.dialogue))\n",
        "    references_val.append(row.section_text)\n",
        "\n",
        "# Apply same filtering as training data\n",
        "prompts_val, references_val = filter_data_by_length(\n",
        "    prompts_val, references_val, max_input_tokens=1500\n",
        ")\n",
        "\n",
        "# ---------- 1. Evaluate the fine-tuned model first ----------\n",
        "fine_outputs, fine_scores, fine_metrics = evaluate_model(\n",
        "    gemma_lm, prompts_val, references_val, \"Fine-tuned\"\n",
        ")\n",
        "\n",
        "# Free GPU memory held by the tuned checkpoint\n",
        "del gemma_lm\n",
        "gc.collect()\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "# ---------- 2. Evaluate a fresh base model ----------\n",
        "base_model = keras_hub.models.Gemma3CausalLM.from_preset(\"gemma3_instruct_1b\")\n",
        "compile_with_sampler(base_model, k=5, seed=SEED)\n",
        "\n",
        "base_outputs, base_scores, base_metrics = evaluate_model(\n",
        "    base_model, prompts_val, references_val, \"Base\"\n",
        ")\n",
        "\n",
        "# ---------- 3. Summarise the improvements ----------\n",
        "rows = []\n",
        "for metric in [\"rouge1\", \"rouge2\", \"rougeL\"]:\n",
        "    base_vals = base_scores[metric]\n",
        "    fine_vals = fine_scores[metric]\n",
        "    delta = np.mean(fine_vals) - np.mean(base_vals)\n",
        "    t_stat, p_val = stats.ttest_rel(fine_vals, base_vals)\n",
        "    rows.append((metric.upper(), np.mean(base_vals), np.mean(fine_vals), delta, p_val))\n",
        "\n",
        "results_df = pd.DataFrame(\n",
        "    rows, columns=[\"Metric\", \"Base\", \"Fine-tuned\", \"Î”\", \"p-value\"]\n",
        ").set_index(\"Metric\")\n",
        "\n",
        "results_df.style.format(\"{:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4b74dd7",
      "metadata": {
        "id": "d4b74dd7"
      },
      "source": [
        "### ğŸ“ Qualitative sample (first 2 rows)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bd5b4b3a",
      "metadata": {
        "id": "bd5b4b3a"
      },
      "outputs": [],
      "source": [
        "for i in range(2):\n",
        "    print(f\"\\nâ€”â€”â€” Example {i+1} â€”â€”â€”\")\n",
        "    print(\"REFERENCE:\\n\", references_val[i][:800], \"\\n\")\n",
        "    print(\"BASE MODEL:\\n\", base_outputs[i][:800], \"\\n\")\n",
        "    print(\"FINEâ€‘TUNED:\\n\", fine_outputs[i][:800])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8813fa95",
      "metadata": {
        "id": "8813fa95"
      },
      "source": [
        "## ğŸ›ï¸ Playground\n",
        "\n",
        "Type / paste a medical conversation below and click **Summarise** to see the fineâ€‘tuned model in action.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a01428b",
      "metadata": {
        "id": "4a01428b"
      },
      "outputs": [],
      "source": [
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "\n",
        "# Load the fine-tuned model with the LoRA-adapted weights\n",
        "fresh_lm = keras_hub.models.Gemma3CausalLM.from_preset(\"gemma3_instruct_1b\")\n",
        "fresh_lm.backbone.enable_lora(rank=8, trainable=False)   # must match the saved rank\n",
        "fresh_lm.backbone.load_lora_weights(\"lora_rank8.weights.lora.h5\")\n",
        "\n",
        "input_box = widgets.Textarea(\n",
        "    value=\"Doctor: How do you feel?\\nPatient: A bit dizzy.\",\n",
        "    placeholder=\"Paste dialogueâ€¦\",\n",
        "    description=\"Dialogue:\",\n",
        "    layout=widgets.Layout(width='100%', height='140px')\n",
        ")\n",
        "\n",
        "button = widgets.Button(description=\"Summarise\", button_style='primary')\n",
        "output = widgets.Output()\n",
        "\n",
        "def on_click(_):\n",
        "    output.clear_output()\n",
        "    prompt = TEMPLATE.format(\n",
        "        instruction=\"Summarise the dialogue into a concise clinical note.\",\n",
        "        dialogue=input_box.value\n",
        "    )\n",
        "    with output:\n",
        "        result = safe_generate(fresh_lm, prompt, max_length=512)  # Consistent with evaluation\n",
        "        print(result)\n",
        "\n",
        "button.on_click(on_click)\n",
        "display(input_box, button, output)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "01fe61fb",
      "metadata": {
        "id": "01fe61fb"
      },
      "source": [
        "<details>\n",
        "<summary><strong>Further work & deployment notes</strong></summary>\n",
        "\n",
        "* **Data scalingÂ ğŸ—‚ï¸** â€“ training on the full 1.7â€¯k rows adds ~0.4â€¯ROUGEâ€‘L.  \n",
        "* **Hyperâ€‘parametersÂ âš™ï¸** â€“ try `rank=16` LoRA or 5Â epochs for marginal gains.  \n",
        "* **InferenceÂ ğŸš€** â€“ export merged weights (`gemma_lm.backbone.merge_lora_weights()`) for faster CPU serving.  \n",
        "* **SafetyÂ ğŸ”** â€“ run generated notes through clinicalâ€‘NLP validation & redaction before use.  \n",
        "\n",
        "</details>\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
